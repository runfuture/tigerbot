{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3Y83z1zjv8MSbCtclK47w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8cbf19a8230d496492dbcd721ab9fe9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7001ddfcf2c440ceafe188c883677a60",
              "IPY_MODEL_71c7020fac87476bad329b910089c8fc",
              "IPY_MODEL_fed0a26045ff4d3c93299abc36227533"
            ],
            "layout": "IPY_MODEL_cf1d0475bfa042c8819ef0864db9ca08"
          }
        },
        "7001ddfcf2c440ceafe188c883677a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6030f504be484b1ea2b33c9f90c77239",
            "placeholder": "​",
            "style": "IPY_MODEL_fa065a92c2214b588c6846f895a8d38f",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "71c7020fac87476bad329b910089c8fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcfb3d4b310947e7b1a87a0bcf1f9866",
            "max": 283,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c379001cbd2d445eb7a83758c0df38ed",
            "value": 283
          }
        },
        "fed0a26045ff4d3c93299abc36227533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9769fa7821514861b3cb7b3a0e01ec9a",
            "placeholder": "​",
            "style": "IPY_MODEL_ec7e52c4518f4e2b941b8716c81c53f1",
            "value": " 283/283 [00:00&lt;00:00, 20.9kB/s]"
          }
        },
        "cf1d0475bfa042c8819ef0864db9ca08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6030f504be484b1ea2b33c9f90c77239": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa065a92c2214b588c6846f895a8d38f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcfb3d4b310947e7b1a87a0bcf1f9866": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c379001cbd2d445eb7a83758c0df38ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9769fa7821514861b3cb7b3a0e01ec9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec7e52c4518f4e2b941b8716c81c53f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d11dead90bc348b5b8acb7db30ac81e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77c45794190b4073846d5fecc21a82c4",
              "IPY_MODEL_91c76278503f4c9b87b4a6337a8cba23",
              "IPY_MODEL_752ccd08dd254efca59df6c9da0c896d"
            ],
            "layout": "IPY_MODEL_58ce3dbf8f074585bddce2a10b725ea5"
          }
        },
        "77c45794190b4073846d5fecc21a82c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_291419c7a0424b64834f733214b0363c",
            "placeholder": "​",
            "style": "IPY_MODEL_b0349369fd4a42119cd5dbe5b40b3d4f",
            "value": "Downloading tokenizer.json: 100%"
          }
        },
        "91c76278503f4c9b87b4a6337a8cba23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d61ec87fff24355bada9abb9ef4fa16",
            "max": 14500838,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_465a150a78144d22a1700d8a8ca3cf73",
            "value": 14500838
          }
        },
        "752ccd08dd254efca59df6c9da0c896d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcb0a64b308a4d66b7a144c69cac9a53",
            "placeholder": "​",
            "style": "IPY_MODEL_31bd7228c1134007b2808fd515b5b213",
            "value": " 14.5M/14.5M [00:00&lt;00:00, 18.3MB/s]"
          }
        },
        "58ce3dbf8f074585bddce2a10b725ea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "291419c7a0424b64834f733214b0363c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0349369fd4a42119cd5dbe5b40b3d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d61ec87fff24355bada9abb9ef4fa16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "465a150a78144d22a1700d8a8ca3cf73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bcb0a64b308a4d66b7a144c69cac9a53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31bd7228c1134007b2808fd515b5b213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d6fa9649c904cf49f65638ac11b4b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_205bea284dc04634991de1fe7f152b8c",
              "IPY_MODEL_8fe684f40da24ba29419f74e53ebd7ba",
              "IPY_MODEL_c5f26bd1c8784d2aac455b806fd1a407"
            ],
            "layout": "IPY_MODEL_7807eea052f04be39b417f753b5b15e4"
          }
        },
        "205bea284dc04634991de1fe7f152b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_712f0e976b1045eeacd1767dd348e60a",
            "placeholder": "​",
            "style": "IPY_MODEL_1dcf50602cee4991832bbda3d5097cf2",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "8fe684f40da24ba29419f74e53ebd7ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c04328156464a1bbde0cdca27173b64",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df1f28ebf91d4526a6a109e7e32da24f",
            "value": 190
          }
        },
        "c5f26bd1c8784d2aac455b806fd1a407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e0e3c38cd294b7882b690ecea84d00a",
            "placeholder": "​",
            "style": "IPY_MODEL_f70e268d695e4934a120d7393103cd4d",
            "value": " 190/190 [00:00&lt;00:00, 16.3kB/s]"
          }
        },
        "7807eea052f04be39b417f753b5b15e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "712f0e976b1045eeacd1767dd348e60a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dcf50602cee4991832bbda3d5097cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c04328156464a1bbde0cdca27173b64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df1f28ebf91d4526a6a109e7e32da24f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e0e3c38cd294b7882b690ecea84d00a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f70e268d695e4934a120d7393103cd4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/runfuture/tigerbot/blob/main/test_tigerbot_7b_sft_4bit_128g.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n",
        "!pip install transformers\n",
        "!pip install triton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s02ISYvk0V6c",
        "outputId": "bec5c710-c0a6-4874-fb6d-0c6ce4495cd1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton) (3.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from triton) (2.0.1+cu118)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton) (16.0.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->triton) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->triton) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->triton) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->triton) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->triton) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->triton) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TigerResearch/tigerbot-7b-sft-4bit-128g/resolve/main/tigerbot-7b-4bit-128g.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXnJMbnNBbVp",
        "outputId": "7bfaefc6-0e28-4bf3-cd99-0241c43244f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-07 13:22:22--  https://huggingface.co/TigerResearch/tigerbot-7b-sft-4bit-128g/resolve/main/tigerbot-7b-4bit-128g.pt\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.170.36, 18.172.170.70, 18.172.170.14, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.170.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/73/89/73895d0bc15c24f2d617c78999d85fe3cc56a28b718a5ab4e8d045eba9ae92d5/d85ab05241957f7b1c8a2d83a25af986879108d67fe79316ce685412d7b7dc3d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tigerbot-7b-4bit-128g.pt%3B+filename%3D%22tigerbot-7b-4bit-128g.pt%22%3B&Expires=1686403342&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzczLzg5LzczODk1ZDBiYzE1YzI0ZjJkNjE3Yzc4OTk5ZDg1ZmUzY2M1NmEyOGI3MThhNWFiNGU4ZDA0NWViYTlhZTkyZDUvZDg1YWIwNTI0MTk1N2Y3YjFjOGEyZDgzYTI1YWY5ODY4NzkxMDhkNjdmZTc5MzE2Y2U2ODU0MTJkN2I3ZGMzZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODY0MDMzNDJ9fX1dfQ__&Signature=g0XT63TK84n6dUQ4TIoMJhsbJSeMPWaYsIDDP0tI-gl33Vw2czbcUEpCnxd1nVfcLwuRcXT8D3scsqBapLobgWybSOCCLduK5-bO8jJo2zlsbDiKsEtSV0tZ%7EASrmKIfHsNvLpZhGWWyIDuhlXvE41lOvYMmym3EMXWP4A2LP2M5COuyt4ilCRR4korsWxBy0nKj0sH1rCwqRiK34HGIZTb6%7ElCYB0GZRQ0nGWeCicj8avTG8yTVmcvYYGlzDuTZgNdVJQVWnts6hYXhS5IHEgxzjDwyVYlQQYqvvh6kvCWYp0GI4nKMATfIOnLbcaxoC-x4tYGC8Z4SOSpuWRG2Ag__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-06-07 13:22:22--  https://cdn-lfs.huggingface.co/repos/73/89/73895d0bc15c24f2d617c78999d85fe3cc56a28b718a5ab4e8d045eba9ae92d5/d85ab05241957f7b1c8a2d83a25af986879108d67fe79316ce685412d7b7dc3d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tigerbot-7b-4bit-128g.pt%3B+filename%3D%22tigerbot-7b-4bit-128g.pt%22%3B&Expires=1686403342&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzczLzg5LzczODk1ZDBiYzE1YzI0ZjJkNjE3Yzc4OTk5ZDg1ZmUzY2M1NmEyOGI3MThhNWFiNGU4ZDA0NWViYTlhZTkyZDUvZDg1YWIwNTI0MTk1N2Y3YjFjOGEyZDgzYTI1YWY5ODY4NzkxMDhkNjdmZTc5MzE2Y2U2ODU0MTJkN2I3ZGMzZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODY0MDMzNDJ9fX1dfQ__&Signature=g0XT63TK84n6dUQ4TIoMJhsbJSeMPWaYsIDDP0tI-gl33Vw2czbcUEpCnxd1nVfcLwuRcXT8D3scsqBapLobgWybSOCCLduK5-bO8jJo2zlsbDiKsEtSV0tZ%7EASrmKIfHsNvLpZhGWWyIDuhlXvE41lOvYMmym3EMXWP4A2LP2M5COuyt4ilCRR4korsWxBy0nKj0sH1rCwqRiK34HGIZTb6%7ElCYB0GZRQ0nGWeCicj8avTG8yTVmcvYYGlzDuTZgNdVJQVWnts6hYXhS5IHEgxzjDwyVYlQQYqvvh6kvCWYp0GI4nKMATfIOnLbcaxoC-x4tYGC8Z4SOSpuWRG2Ag__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.65.229.35, 18.65.229.83, 18.65.229.16, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.65.229.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5198372983 (4.8G) [binary/octet-stream]\n",
            "Saving to: ‘tigerbot-7b-4bit-128g.pt’\n",
            "\n",
            "tigerbot-7b-4bit-12 100%[===================>]   4.84G  47.9MB/s    in 95s     \n",
            "\n",
            "2023-06-07 13:23:57 (52.3 MB/s) - ‘tigerbot-7b-4bit-128g.pt’ saved [5198372983/5198372983]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lePlR0LU0TGb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#import quant\n",
        "import glob\n",
        "from collections import OrderedDict\n",
        "\n",
        "#from utils import find_layers, DEV\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, modeling_utils\n",
        "from accelerate.utils import get_balanced_memory\n",
        "from accelerate import infer_auto_device_map, dispatch_model\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "DEV = torch.device('cuda:0')\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import Dict\n",
        "import time\n",
        "import builtins\n",
        "import triton\n",
        "\n",
        "from torch.cuda.amp import custom_bwd, custom_fwd\n",
        "\n",
        "class Autotuner(triton.KernelInterface):\n",
        "\n",
        "    def __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by: Dict = None, nearest_power_of_two: bool = False):\n",
        "        '''\n",
        "\t\t:param prune_configs_by: a dict of functions that are used to prune configs, fields:\n",
        "\t\t\t'perf_model': performance model used to predicate running time with different configs, returns running time\n",
        "\t\t\t'top_k': number of configs to bench\n",
        "\t\t\t'prune_num_stages_by'(optional): a function used to prune num_stages. It take configs:List[Config] as its input, and returns pruned configs.\n",
        "\t\t\t'nearest_power_of_two'(optional): whether to round key arguments to the nearest power of two when caching tuning results\n",
        "\t\t'''\n",
        "        if not configs:\n",
        "            self.configs = [triton.Config({}, num_warps=4, num_stages=2)]\n",
        "        else:\n",
        "            self.configs = configs\n",
        "        self.key_idx = [arg_names.index(k) for k in key]\n",
        "        self.nearest_power_of_two = nearest_power_of_two\n",
        "        self.cache = {}\n",
        "        # hook to reset all required tensor to zeros before relaunching a kernel\n",
        "        self.hook = lambda args: 0\n",
        "        if reset_to_zero is not None:\n",
        "            self.reset_idx = [arg_names.index(k) for k in reset_to_zero]\n",
        "\n",
        "            def _hook(args):\n",
        "                for i in self.reset_idx:\n",
        "                    args[i].zero_()\n",
        "\n",
        "            self.hook = _hook\n",
        "        self.arg_names = arg_names\n",
        "        # prune configs\n",
        "        if prune_configs_by:\n",
        "            perf_model, top_k = prune_configs_by['perf_model'], prune_configs_by['top_k']\n",
        "            if 'early_config_prune' in prune_configs_by:\n",
        "                early_config_prune = prune_configs_by['early_config_prune']\n",
        "        else:\n",
        "            perf_model, top_k, early_config_prune = None, None, None\n",
        "        self.perf_model, self.configs_top_k = perf_model, top_k\n",
        "        self.early_config_prune = early_config_prune\n",
        "        self.fn = fn\n",
        "\n",
        "    def _bench(self, *args, config, **meta):\n",
        "        # check for conflicts, i.e. meta-parameters both provided\n",
        "        # as kwargs and by the autotuner\n",
        "        conflicts = meta.keys() & config.kwargs.keys()\n",
        "        if conflicts:\n",
        "            raise ValueError(f\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\n",
        "                             \" Make sure that you don't re-define auto-tuned symbols.\")\n",
        "        # augment meta-parameters with tunable ones\n",
        "        current = dict(meta, **config.kwargs)\n",
        "\n",
        "        def kernel_call():\n",
        "            if config.pre_hook:\n",
        "                config.pre_hook(self.nargs)\n",
        "            self.hook(args)\n",
        "            self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n",
        "\n",
        "        try:\n",
        "            # In testings using only 40 reps seems to be close enough and it appears to be what PyTorch uses\n",
        "            # PyTorch also sets fast_flush to True, but I didn't see any speedup so I'll leave the default\n",
        "            return triton.testing.do_bench(kernel_call, percentiles=(0.5, 0.2, 0.8), rep=40)\n",
        "        except triton.compiler.OutOfResources:\n",
        "            return (float('inf'), float('inf'), float('inf'))\n",
        "\n",
        "    def run(self, *args, **kwargs):\n",
        "        self.nargs = dict(zip(self.arg_names, args))\n",
        "        if len(self.configs) > 1:\n",
        "            key = tuple(args[i] for i in self.key_idx)\n",
        "\n",
        "            # This reduces the amount of autotuning by rounding the keys to the nearest power of two\n",
        "            # In my testing this gives decent results, and greatly reduces the amount of tuning required\n",
        "            if self.nearest_power_of_two:\n",
        "                key = tuple([2**int(math.log2(x) + 0.5) for x in key])\n",
        "\n",
        "            if key not in self.cache:\n",
        "                # prune configs\n",
        "                pruned_configs = self.prune_configs(kwargs)\n",
        "                bench_start = time.time()\n",
        "                timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n",
        "                bench_end = time.time()\n",
        "                self.bench_time = bench_end - bench_start\n",
        "                self.cache[key] = builtins.min(timings, key=timings.get)\n",
        "                self.hook(args)\n",
        "                self.configs_timings = timings\n",
        "            config = self.cache[key]\n",
        "        else:\n",
        "            config = self.configs[0]\n",
        "        self.best_config = config\n",
        "        if config.pre_hook is not None:\n",
        "            config.pre_hook(self.nargs)\n",
        "        return self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n",
        "\n",
        "    def prune_configs(self, kwargs):\n",
        "        pruned_configs = self.configs\n",
        "        if self.early_config_prune:\n",
        "            pruned_configs = self.early_config_prune(self.configs, self.nargs)\n",
        "        if self.perf_model:\n",
        "            top_k = self.configs_top_k\n",
        "            if isinstance(top_k, float) and top_k <= 1.0:\n",
        "                top_k = int(len(self.configs) * top_k)\n",
        "            if len(pruned_configs) > top_k:\n",
        "                est_timing = {config: self.perf_model(**self.nargs, **kwargs, **config.kwargs, num_stages=config.num_stages, num_warps=config.num_warps) for config in pruned_configs}\n",
        "                pruned_configs = sorted(est_timing.keys(), key=lambda x: est_timing[x])[:top_k]\n",
        "        return pruned_configs\n",
        "\n",
        "    def warmup(self, *args, **kwargs):\n",
        "        self.nargs = dict(zip(self.arg_names, args))\n",
        "        for config in self.prune_configs(kwargs):\n",
        "            self.fn.warmup(\n",
        "                *args,\n",
        "                num_warps=config.num_warps,\n",
        "                num_stages=config.num_stages,\n",
        "                **kwargs,\n",
        "                **config.kwargs,\n",
        "            )\n",
        "        self.nargs = None\n",
        "\n",
        "\n",
        "def autotune(configs, key, prune_configs_by=None, reset_to_zero=None, nearest_power_of_two=False):\n",
        "    \"\"\"\n",
        "\tDecorator for auto-tuning a :code:`triton.jit`'d function.\n",
        "\t.. highlight:: python\n",
        "\t.. code-block:: python\n",
        "\t\t@triton.autotune(configs=[\n",
        "\t\t\ttriton.Config(meta={'BLOCK_SIZE': 128}, num_warps=4),\n",
        "\t\t\ttriton.Config(meta={'BLOCK_SIZE': 1024}, num_warps=8),\n",
        "\t\t\t],\n",
        "\t\t\tkey=['x_size'] # the two above configs will be evaluated anytime\n",
        "\t\t\t\t\t\t\t# the value of x_size changes\n",
        "\t\t)\n",
        "\t\t@triton.jit\n",
        "\t\tdef kernel(x_ptr, x_size, **META):\n",
        "\t\t\tBLOCK_SIZE = META['BLOCK_SIZE']\n",
        "\t:note: When all the configurations are evaluated, the kernel will run multiple time.\n",
        "\t\t\tThis means that whatever value the kernel updates will be updated multiple times.\n",
        "\t\t\tTo avoid this undesired behavior, you can use the `reset_to_zero` argument, which\n",
        "\t\t\treset the value of the provided tensor to `zero` before running any configuration.\n",
        "\t:param configs: a list of :code:`triton.Config` objects\n",
        "\t:type configs: list[triton.Config]\n",
        "\t:param key: a list of argument names whose change in value will trigger the evaluation of all provided configs.\n",
        "\t:type key: list[str]\n",
        "\t:param prune_configs_by: a dict of functions that are used to prune configs, fields:\n",
        "\t\t'perf_model': performance model used to predicate running time with different configs, returns running time\n",
        "\t\t'top_k': number of configs to bench\n",
        "\t\t'early_config_prune'(optional): a function used to do early prune (eg, num_stages). It take configs:List[Config] as its input, and returns pruned configs.\n",
        "\t:param reset_to_zero: a list of argument names whose value will be reset to zero before evaluating any configs.\n",
        "\t:type reset_to_zero: list[str]\n",
        "\t\"\"\"\n",
        "\n",
        "    def decorator(fn):\n",
        "        return Autotuner(fn, fn.arg_names, configs, key, reset_to_zero, prune_configs_by, nearest_power_of_two)\n",
        "\n",
        "    return decorator\n",
        "\n",
        "\n",
        "def matmul248_kernel_config_pruner(configs, nargs):\n",
        "    \"\"\"\n",
        "    The main purpose of this function is to shrink BLOCK_SIZE_* when the corresponding dimension is smaller.\n",
        "    \"\"\"\n",
        "    m = max(2**int(math.ceil(math.log2(nargs['M']))), 16)\n",
        "    n = max(2**int(math.ceil(math.log2(nargs['N']))), 16)\n",
        "    k = max(2**int(math.ceil(math.log2(nargs['K']))), 16)\n",
        "\n",
        "    used = set()\n",
        "    for config in configs:\n",
        "        block_size_m = min(m, config.kwargs['BLOCK_SIZE_M'])\n",
        "        block_size_n = min(n, config.kwargs['BLOCK_SIZE_N'])\n",
        "        block_size_k = min(k, config.kwargs['BLOCK_SIZE_K'])\n",
        "        group_size_m = config.kwargs['GROUP_SIZE_M']\n",
        "\n",
        "        if (block_size_m, block_size_n, block_size_k, group_size_m, config.num_stages, config.num_warps) in used:\n",
        "            continue\n",
        "\n",
        "        used.add((block_size_m, block_size_n, block_size_k, group_size_m, config.num_stages, config.num_warps))\n",
        "        yield triton.Config({\n",
        "            'BLOCK_SIZE_M': block_size_m,\n",
        "            'BLOCK_SIZE_N': block_size_n,\n",
        "            'BLOCK_SIZE_K': block_size_k,\n",
        "            'GROUP_SIZE_M': group_size_m\n",
        "        },\n",
        "                            num_stages=config.num_stages,\n",
        "                            num_warps=config.num_warps)\n",
        "\n",
        "#try:\n",
        "if True:\n",
        "    import triton\n",
        "    import triton.language as tl\n",
        "    #from . import custom_autotune\n",
        "\n",
        "    # code based https://github.com/fpgaminer/GPTQ-triton\n",
        "    #@custom_autotune.autotune(\n",
        "    @autotune(\n",
        "        configs=[\n",
        "            triton.Config({\n",
        "                'BLOCK_SIZE_M': 64,\n",
        "                'BLOCK_SIZE_N': 256,\n",
        "                'BLOCK_SIZE_K': 32,\n",
        "                'GROUP_SIZE_M': 8\n",
        "            }, num_stages=4, num_warps=4),\n",
        "            triton.Config({\n",
        "                'BLOCK_SIZE_M': 128,\n",
        "                'BLOCK_SIZE_N': 128,\n",
        "                'BLOCK_SIZE_K': 32,\n",
        "                'GROUP_SIZE_M': 8\n",
        "            }, num_stages=4, num_warps=4),\n",
        "            triton.Config({\n",
        "                'BLOCK_SIZE_M': 64,\n",
        "                'BLOCK_SIZE_N': 128,\n",
        "                'BLOCK_SIZE_K': 32,\n",
        "                'GROUP_SIZE_M': 8\n",
        "            }, num_stages=4, num_warps=4),\n",
        "            triton.Config({\n",
        "                'BLOCK_SIZE_M': 128,\n",
        "                'BLOCK_SIZE_N': 32,\n",
        "                'BLOCK_SIZE_K': 32,\n",
        "                'GROUP_SIZE_M': 8\n",
        "            }, num_stages=4, num_warps=4),\n",
        "            triton.Config({\n",
        "                'BLOCK_SIZE_M': 64,\n",
        "                'BLOCK_SIZE_N': 64,\n",
        "                'BLOCK_SIZE_K': 32,\n",
        "                'GROUP_SIZE_M': 8\n",
        "            }, num_stages=4, num_warps=4),\n",
        "            triton.Config({\n",
        "                'BLOCK_SIZE_M': 64,\n",
        "                'BLOCK_SIZE_N': 128,\n",
        "                'BLOCK_SIZE_K': 32,\n",
        "                'GROUP_SIZE_M': 8\n",
        "            }, num_stages=2, num_warps=8),\n",
        "            triton.Config({\n",
        "                'BLOCK_SIZE_M': 64,\n",
        "                'BLOCK_SIZE_N': 64,\n",
        "                'BLOCK_SIZE_K': 64,\n",
        "                'GROUP_SIZE_M': 8\n",
        "            }, num_stages=3, num_warps=8),\n",
        "            triton.Config({\n",
        "                'BLOCK_SIZE_M': 32,\n",
        "                'BLOCK_SIZE_N': 32,\n",
        "                'BLOCK_SIZE_K': 128,\n",
        "                'GROUP_SIZE_M': 8\n",
        "            }, num_stages=2, num_warps=4),\n",
        "        ],\n",
        "        key=['M', 'N', 'K'],\n",
        "        nearest_power_of_two=True,\n",
        "        prune_configs_by={\n",
        "            #'early_config_prune': custom_autotune.matmul248_kernel_config_pruner,\n",
        "            'early_config_prune': matmul248_kernel_config_pruner,\n",
        "            'perf_model': None,\n",
        "            'top_k': None,\n",
        "        },\n",
        "    )\n",
        "    @triton.jit\n",
        "    def matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, g_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_scales, stride_zeros,\n",
        "                          BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):\n",
        "        \"\"\"\n",
        "        Compute the matrix multiplication C = A x B.\n",
        "        A is of shape (M, K) float16\n",
        "        B is of shape (K//8, N) int32\n",
        "        C is of shape (M, N) float16\n",
        "        scales is of shape (G, N) float16\n",
        "        zeros is of shape (G, N) float16\n",
        "        g_ptr is of shape (K) int32 \n",
        "        \"\"\"\n",
        "        infearure_per_bits = 32 // bits\n",
        "\n",
        "        pid = tl.program_id(axis=0)\n",
        "        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
        "        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
        "        num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n",
        "        num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
        "        group_id = pid // num_pid_in_group\n",
        "        first_pid_m = group_id * GROUP_SIZE_M\n",
        "        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
        "        pid_m = first_pid_m + (pid % group_size_m)\n",
        "        pid_n = (pid % num_pid_in_group) // group_size_m\n",
        "\n",
        "        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
        "        offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
        "        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n",
        "        a_mask = (offs_am[:, None] < M)\n",
        "        # b_ptrs is set up such that it repeats elements along the K axis 8 times\n",
        "        b_ptrs = b_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)  # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n",
        "        g_ptrs = g_ptr + offs_k\n",
        "        # shifter is used to extract the N bits of each element in the 32-bit word from B\n",
        "        scales_ptrs = scales_ptr + offs_bn[None, :]\n",
        "        zeros_ptrs = zeros_ptr + (offs_bn[None, :] // infearure_per_bits)\n",
        "\n",
        "        shifter = (offs_k % infearure_per_bits) * bits\n",
        "        zeros_shifter = (offs_bn % infearure_per_bits) * bits\n",
        "        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
        "\n",
        "        for k in range(0, num_pid_k):\n",
        "            g_idx = tl.load(g_ptrs)\n",
        "\n",
        "            # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n",
        "            scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n",
        "            zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n",
        "\n",
        "            zeros = (zeros >> zeros_shifter[None, :]) & maxq\n",
        "            zeros = (zeros + 1)\n",
        "\n",
        "            a = tl.load(a_ptrs, mask=a_mask, other=0.)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n",
        "            b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n",
        "\n",
        "            # Now we need to unpack b (which is N-bit values) into 32-bit values\n",
        "            b = (b >> shifter[:, None]) & maxq  # Extract the N-bit values\n",
        "            b = (b - zeros) * scales  # Scale and shift\n",
        "\n",
        "            accumulator += tl.dot(a, b)\n",
        "            a_ptrs += BLOCK_SIZE_K\n",
        "            b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk\n",
        "            g_ptrs += BLOCK_SIZE_K\n",
        "\n",
        "        c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]\n",
        "        c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n",
        "        tl.store(c_ptrs, accumulator, mask=c_mask)\n",
        "\n",
        "    #@custom_autotune.autotune(configs=[\n",
        "    @autotune(configs=[\n",
        "        triton.Config({\n",
        "            'BLOCK_SIZE_M': 64,\n",
        "            'BLOCK_SIZE_N': 32,\n",
        "            'BLOCK_SIZE_K': 256,\n",
        "            'GROUP_SIZE_M': 8\n",
        "        }, num_stages=4, num_warps=4),\n",
        "        triton.Config({\n",
        "            'BLOCK_SIZE_M': 128,\n",
        "            'BLOCK_SIZE_N': 32,\n",
        "            'BLOCK_SIZE_K': 128,\n",
        "            'GROUP_SIZE_M': 8\n",
        "        }, num_stages=4, num_warps=4),\n",
        "        triton.Config({\n",
        "            'BLOCK_SIZE_M': 64,\n",
        "            'BLOCK_SIZE_N': 32,\n",
        "            'BLOCK_SIZE_K': 128,\n",
        "            'GROUP_SIZE_M': 8\n",
        "        }, num_stages=4, num_warps=4),\n",
        "        triton.Config({\n",
        "            'BLOCK_SIZE_M': 128,\n",
        "            'BLOCK_SIZE_N': 32,\n",
        "            'BLOCK_SIZE_K': 32,\n",
        "            'GROUP_SIZE_M': 8\n",
        "        }, num_stages=4, num_warps=4),\n",
        "        triton.Config({\n",
        "            'BLOCK_SIZE_M': 64,\n",
        "            'BLOCK_SIZE_N': 32,\n",
        "            'BLOCK_SIZE_K': 64,\n",
        "            'GROUP_SIZE_M': 8\n",
        "        }, num_stages=4, num_warps=4),\n",
        "        triton.Config({\n",
        "            'BLOCK_SIZE_M': 64,\n",
        "            'BLOCK_SIZE_N': 32,\n",
        "            'BLOCK_SIZE_K': 128,\n",
        "            'GROUP_SIZE_M': 8\n",
        "        }, num_stages=2, num_warps=8),\n",
        "        triton.Config({\n",
        "            'BLOCK_SIZE_M': 64,\n",
        "            'BLOCK_SIZE_N': 64,\n",
        "            'BLOCK_SIZE_K': 64,\n",
        "            'GROUP_SIZE_M': 8\n",
        "        }, num_stages=3, num_warps=8),\n",
        "        triton.Config({\n",
        "            'BLOCK_SIZE_M': 32,\n",
        "            'BLOCK_SIZE_N': 128,\n",
        "            'BLOCK_SIZE_K': 32,\n",
        "            'GROUP_SIZE_M': 8\n",
        "        }, num_stages=2, num_warps=4),\n",
        "    ],\n",
        "                              key=['M', 'N', 'K'],\n",
        "                              nearest_power_of_two=True)\n",
        "    @triton.jit\n",
        "    def transpose_matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, g_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_scales,\n",
        "                                    stride_zeros, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):\n",
        "        \"\"\"\n",
        "        Compute the matrix multiplication C = A x B.\n",
        "        A is of shape (M, N) float16\n",
        "        B is of shape (K//8, N) int32\n",
        "        C is of shape (M, K) float16\n",
        "        scales is of shape (G, N) float16\n",
        "        zeros is of shape (G, N) float16\n",
        "        g_ptr is of shape (K) int32 \n",
        "        \"\"\"\n",
        "        infearure_per_bits = 32 // bits\n",
        "\n",
        "        pid = tl.program_id(axis=0)\n",
        "        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
        "        num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n",
        "        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
        "        num_pid_in_group = GROUP_SIZE_M * num_pid_k\n",
        "        group_id = pid // num_pid_in_group\n",
        "        first_pid_m = group_id * GROUP_SIZE_M\n",
        "        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
        "        pid_m = first_pid_m + (pid % group_size_m)\n",
        "        pid_k = (pid % num_pid_in_group) // group_size_m\n",
        "\n",
        "        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "        offs_bk = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
        "        offs_n = tl.arange(0, BLOCK_SIZE_N)\n",
        "        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_n[None, :] * stride_ak)  # (BLOCK_SIZE_M, BLOCK_SIZE_N)\n",
        "        a_mask = (offs_am[:, None] < M)\n",
        "        # b_ptrs is set up such that it repeats elements along the K axis 8 times\n",
        "        b_ptrs = b_ptr + ((offs_bk[:, None] // infearure_per_bits) * stride_bk + offs_n[None, :] * stride_bn)  # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n",
        "        g_ptrs = g_ptr + offs_bk\n",
        "        g_idx = tl.load(g_ptrs)\n",
        "\n",
        "        # shifter is used to extract the N bits of each element in the 32-bit word from B\n",
        "        scales_ptrs = scales_ptr + offs_n[None, :] + g_idx[:, None] * stride_scales\n",
        "        zeros_ptrs = zeros_ptr + (offs_n[None, :] // infearure_per_bits) + g_idx[:, None] * stride_zeros\n",
        "\n",
        "        shifter = (offs_bk % infearure_per_bits) * bits\n",
        "        zeros_shifter = (offs_n % infearure_per_bits) * bits\n",
        "        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n",
        "\n",
        "        for n in range(0, num_pid_n):\n",
        "            # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n",
        "            scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n",
        "            zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n",
        "\n",
        "            zeros = (zeros >> zeros_shifter[None, :]) & maxq\n",
        "            zeros = (zeros + 1)\n",
        "\n",
        "            a = tl.load(a_ptrs, mask=a_mask, other=0.)  # (BLOCK_SIZE_M, BLOCK_SIZE_N)\n",
        "            b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n",
        "\n",
        "            # Now we need to unpack b (which is N-bit values) into 32-bit values\n",
        "            b = (b >> shifter[:, None]) & maxq  # Extract the N-bit values\n",
        "            b = (b - zeros) * scales  # Scale and shift\n",
        "            b = tl.trans(b)\n",
        "\n",
        "            accumulator += tl.dot(a, b)\n",
        "            a_ptrs += BLOCK_SIZE_N\n",
        "            b_ptrs += BLOCK_SIZE_N\n",
        "            scales_ptrs += BLOCK_SIZE_N\n",
        "            zeros_ptrs += (BLOCK_SIZE_N // infearure_per_bits)\n",
        "\n",
        "        c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bk[None, :]\n",
        "        c_mask = (offs_am[:, None] < M) & (offs_bk[None, :] < K)\n",
        "        tl.store(c_ptrs, accumulator, mask=c_mask)\n",
        "#except:\n",
        "#    print('triton not installed.')\n",
        "\n",
        "\n",
        "def matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
        "    with torch.cuda.device(input.device):\n",
        "        output = torch.empty((input.shape[0], qweight.shape[1]), device=input.device, dtype=torch.float16)\n",
        "        grid = lambda META: (triton.cdiv(input.shape[0], META['BLOCK_SIZE_M']) * triton.cdiv(qweight.shape[1], META['BLOCK_SIZE_N']), )\n",
        "        matmul_248_kernel[grid](input, qweight, output, scales, qzeros, g_idx, input.shape[0], qweight.shape[1], input.shape[1], bits, maxq, input.stride(0), input.stride(1), qweight.stride(0),\n",
        "                                qweight.stride(1), output.stride(0), output.stride(1), scales.stride(0), qzeros.stride(0))\n",
        "        return output\n",
        "\n",
        "\n",
        "def transpose_matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
        "    with torch.cuda.device(input.device):\n",
        "        output_dim = (qweight.shape[0] * 32) // bits\n",
        "        output = torch.empty((input.shape[0], output_dim), device=input.device, dtype=torch.float16)\n",
        "        grid = lambda META: (triton.cdiv(input.shape[0], META['BLOCK_SIZE_M']) * triton.cdiv(output_dim, META['BLOCK_SIZE_K']), )\n",
        "        transpose_matmul_248_kernel[grid](input, qweight, output, scales, qzeros, g_idx, input.shape[0], qweight.shape[1], output_dim, bits, maxq, input.stride(0), input.stride(1), qweight.stride(0),\n",
        "                                          qweight.stride(1), output.stride(0), output.stride(1), scales.stride(0), qzeros.stride(0))\n",
        "        return output\n",
        "\n",
        "\n",
        "class QuantLinearFunction(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    @custom_fwd(cast_inputs=torch.float16)\n",
        "    def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
        "        output = matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq)\n",
        "        ctx.save_for_backward(qweight, scales, qzeros, g_idx)\n",
        "        ctx.bits, ctx.maxq = bits, maxq\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    @custom_bwd\n",
        "    def backward(ctx, grad_output):\n",
        "        qweight, scales, qzeros, g_idx = ctx.saved_tensors\n",
        "        bits, maxq = ctx.bits, ctx.maxq\n",
        "        grad_input = None\n",
        "\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = transpose_matmul248(grad_output, qweight, scales, qzeros, g_idx, bits, maxq)\n",
        "        return grad_input, None, None, None, None, None, None\n",
        "\n",
        "\n",
        "\n",
        "class QuantLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, bits, groupsize, infeatures, outfeatures, bias):\n",
        "        super().__init__()\n",
        "        if bits not in [2, 4, 8]:\n",
        "            raise NotImplementedError(\"Only 2,4,8 bits are supported.\")\n",
        "        self.infeatures = infeatures\n",
        "        self.outfeatures = outfeatures\n",
        "        self.bits = bits\n",
        "        self.maxq = 2**self.bits - 1\n",
        "        self.groupsize = groupsize if groupsize != -1 else infeatures\n",
        "\n",
        "        self.register_buffer('qweight', torch.zeros((infeatures // 32 * self.bits, outfeatures), dtype=torch.int32))\n",
        "        self.register_buffer('qzeros', torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures // 32 * self.bits), dtype=torch.int32))\n",
        "        self.register_buffer('scales', torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures), dtype=torch.float16))\n",
        "        self.register_buffer('g_idx', torch.tensor([i // self.groupsize for i in range(infeatures)], dtype=torch.int32))\n",
        "        if bias:\n",
        "            self.register_buffer('bias', torch.zeros((outfeatures), dtype=torch.float16))\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def pack(self, linear, scales, zeros, g_idx=None):\n",
        "        self.g_idx = g_idx.clone() if g_idx is not None else self.g_idx\n",
        "\n",
        "        scales = scales.t().contiguous()\n",
        "        zeros = zeros.t().contiguous()\n",
        "        scale_zeros = zeros * scales\n",
        "        self.scales = scales.clone().half()\n",
        "        if linear.bias is not None:\n",
        "            self.bias = linear.bias.clone().half()\n",
        "\n",
        "        intweight = []\n",
        "        for idx in range(self.infeatures):\n",
        "            intweight.append(torch.round((linear.weight.data[:, idx] + scale_zeros[self.g_idx[idx]]) / self.scales[self.g_idx[idx]]).to(torch.int)[:, None])\n",
        "        intweight = torch.cat(intweight, dim=1)\n",
        "        intweight = intweight.t().contiguous()\n",
        "        intweight = intweight.numpy().astype(np.uint32)\n",
        "        qweight = np.zeros((intweight.shape[0] // 32 * self.bits, intweight.shape[1]), dtype=np.uint32)\n",
        "        i = 0\n",
        "        row = 0\n",
        "        while row < qweight.shape[0]:\n",
        "            if self.bits in [2, 4, 8]:\n",
        "                for j in range(i, i + (32 // self.bits)):\n",
        "                    qweight[row] |= intweight[j] << (self.bits * (j - i))\n",
        "                i += 32 // self.bits\n",
        "                row += 1\n",
        "            else:\n",
        "                raise NotImplementedError(\"Only 2,4,8 bits are supported.\")\n",
        "\n",
        "        qweight = qweight.astype(np.int32)\n",
        "        self.qweight = torch.from_numpy(qweight)\n",
        "\n",
        "        zeros -= 1\n",
        "        zeros = zeros.numpy().astype(np.uint32)\n",
        "        qzeros = np.zeros((zeros.shape[0], zeros.shape[1] // 32 * self.bits), dtype=np.uint32)\n",
        "        i = 0\n",
        "        col = 0\n",
        "        while col < qzeros.shape[1]:\n",
        "            if self.bits in [2, 4, 8]:\n",
        "                for j in range(i, i + (32 // self.bits)):\n",
        "                    qzeros[:, col] |= zeros[:, j] << (self.bits * (j - i))\n",
        "                i += 32 // self.bits\n",
        "                col += 1\n",
        "            else:\n",
        "                raise NotImplementedError(\"Only 2,4,8 bits are supported.\")\n",
        "\n",
        "        qzeros = qzeros.astype(np.int32)\n",
        "        self.qzeros = torch.from_numpy(qzeros)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_shape = x.shape[:-1] + (self.outfeatures, )\n",
        "        out = QuantLinearFunction.apply(x.reshape(-1, x.shape[-1]), self.qweight, self.scales, self.qzeros, self.g_idx, self.bits, self.maxq)\n",
        "        out = out + self.bias if self.bias is not None else out\n",
        "        return out.reshape(out_shape)\n",
        "\n",
        "\n",
        "\n",
        "def make_quant_linear(module, names, bits, groupsize, name=''):\n",
        "    if isinstance(module, QuantLinear):\n",
        "        return\n",
        "    for attr in dir(module):\n",
        "        tmp = getattr(module, attr)\n",
        "        name1 = name + '.' + attr if name != '' else attr\n",
        "        if name1 in names:\n",
        "            delattr(module, attr)\n",
        "            setattr(module, attr, QuantLinear(bits, groupsize, tmp.in_features, tmp.out_features, tmp.bias is not None))\n",
        "    for name1, child in module.named_children():\n",
        "        make_quant_linear(child, names, bits, groupsize, name + '.' + name1 if name != '' else name1)\n",
        "\n",
        "def autotune_warmup_linear(model, transpose=False):\n",
        "    \"\"\"\n",
        "    Pre-tunes the quantized kernel\n",
        "    \"\"\"\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    kn_values = {}\n",
        "\n",
        "    for _, m in model.named_modules():\n",
        "        if not isinstance(m, QuantLinear):\n",
        "            continue\n",
        "\n",
        "        k = m.infeatures\n",
        "        n = m.outfeatures\n",
        "\n",
        "        if (k, n) not in kn_values:\n",
        "            kn_values[(k, n)] = (m.qweight.cuda(), m.scales.cuda(), m.qzeros.cuda(), m.g_idx.cuda(), m.bits, m.maxq)\n",
        "\n",
        "    print(f'Found {len(kn_values)} unique KN Linear values.')\n",
        "\n",
        "    print('Warming up autotune cache ...')\n",
        "    with torch.no_grad():\n",
        "        for m in tqdm(range(0, 12)):\n",
        "            m = 2**m  # [1, 2048]\n",
        "            for (k, n), (qweight, scales, qzeros, g_idx, bits, maxq) in kn_values.items():\n",
        "                a = torch.randn(m, k, dtype=torch.float16, device='cuda')\n",
        "                matmul248(a, qweight, scales, qzeros, g_idx, bits, maxq)\n",
        "                if transpose:\n",
        "                    a = torch.randn(m, n, dtype=torch.float16, device='cuda')\n",
        "                    transpose_matmul248(a, qweight, scales, qzeros, g_idx, bits, maxq)\n",
        "    del kn_values\n",
        "\n",
        "def find_layers(module, layers=[nn.Conv2d, nn.Linear], name=''):\n",
        "    if type(module) in layers:\n",
        "        return {name: module}\n",
        "    res = {}\n",
        "    for name1, child in module.named_children():\n",
        "        res.update(find_layers(child, layers=layers, name=name + '.' + name1 if name != '' else name1))\n",
        "    return res\n",
        "\n",
        "def load_quant(model, checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True, warmup_autotune=True):\n",
        "    config = AutoConfig.from_pretrained(model)\n",
        "\n",
        "    def noop(*args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    torch.nn.init.kaiming_uniform_ = noop\n",
        "    torch.nn.init.uniform_ = noop\n",
        "    torch.nn.init.normal_ = noop\n",
        "\n",
        "    torch.set_default_dtype(torch.half)\n",
        "    modeling_utils._init_weights = False\n",
        "    torch.set_default_dtype(torch.half)\n",
        "    model = AutoModelForCausalLM.from_config(config)\n",
        "    torch.set_default_dtype(torch.float)\n",
        "    if eval:\n",
        "        model = model.eval()\n",
        "    layers = find_layers(model)\n",
        "    for name in ['lm_head']:\n",
        "        if name in layers:\n",
        "            del layers[name]\n",
        "    #quant.\n",
        "    make_quant_linear(model, layers, wbits, groupsize)\n",
        "\n",
        "    del layers\n",
        "\n",
        "    print('Loading model ...')\n",
        "    if checkpoint.endswith('.safetensors'):\n",
        "        from safetensors.torch import load_file as safe_load\n",
        "        model.load_state_dict(safe_load(checkpoint))\n",
        "    else:\n",
        "        cp_files = glob.glob(checkpoint)\n",
        "        print(cp_files)\n",
        "        if len(cp_files) == 1:\n",
        "            model.load_state_dict(torch.load(checkpoint))\n",
        "        else:\n",
        "            cp_files = sorted(cp_files)\n",
        "            state_dic = OrderedDict({})\n",
        "            for file in cp_files:\n",
        "                state_dic.update(torch.load(file))\n",
        "            model.load_state_dict(state_dic)\n",
        "            del state_dic\n",
        "\n",
        "    if warmup_autotune:\n",
        "        #quant.\n",
        "        autotune_warmup_linear(model, transpose=not (eval))\n",
        "    model.seqlen = 2048\n",
        "    print('Done.')\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#main(model_path='TigerResearch/tigerbot-7b-sft-4bit-128g')\n",
        "#CUDA_VISIBLE_DEVICES=0 python tigerbot_infer.py TigerResearch/tigerbot-7b-sft-4bit-128g --wbits 4 --groupsize 128 --load tigerbot-7b-4bit-128g.pt\n",
        "\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument(\n",
        "    'model', type=str, default='TigerResearch/tigerbot-7b-sft-4bit-128g',\n",
        "    help='llama model to load'\n",
        ")\n",
        "parser.add_argument(\n",
        "    '--wbits', type=int, default=4, choices=[2, 3, 4, 8, 16],\n",
        "    help='#bits to use for quantization; use 16 for evaluating base model.'\n",
        ")\n",
        "parser.add_argument(\n",
        "    '--groupsize', type=int, default=128,\n",
        "    help='Groupsize to use for quantization; default uses full row.'\n",
        ")\n",
        "parser.add_argument(\n",
        "    '--load', type=str, default='tigerbot-7b-4bit-128g.pt',\n",
        "    help='Load quantized model.'\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    '--max_input_length', type=int, default=512,\n",
        "    help='The maximum length of the input prompt.'\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    '--max_generate_length', type=int, default=1024,\n",
        "    help='The maximum length the generated tokens can have. Corresponds to the length of the input prompt + max_new_tokens'\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    '--top_p', type=float, default=0.95,\n",
        "    help='If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.'\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    '--temperature', type=float, default=0.8,\n",
        "    help='The value used to module the next token probabilities.'\n",
        ")\n",
        "parser.add_argument(\n",
        "    '--no_repeat_ngram_size', type=int, default=4,\n",
        "    help=' If set to int > 0, all ngrams of that size can only occur once.'\n",
        ")\n",
        "\n",
        "args = parser.parse_args(['TigerResearch/tigerbot-7b-sft-4bit-128g', '--wbits', '4', '--groupsize', '128', '--load', 'tigerbot-7b-4bit-128g.pt'])\n",
        "\n",
        "if type(args.load) is not str:\n",
        "    args.load = args.load.as_posix()\n",
        "\n",
        "model = load_quant(args.model, args.load, args.wbits, args.groupsize)\n",
        "\n",
        "max_memory = get_balanced_memory(model)\n",
        "device_map = infer_auto_device_map(model, max_memory=max_memory,\n",
        "                                    no_split_module_classes=[\"BloomBlock\"])\n",
        "print(\"Using the following device map for the model:\", device_map)\n",
        "model = dispatch_model(model, device_map=device_map, offload_buffers=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(args.model, padding_side=\"left\", truncation_side='left')\n",
        "tok_ins = \"\\n\\n### Instruction:\\n\"\n",
        "tok_res = \"\\n\\n### Response:\\n\"\n",
        "prompt_input = tok_ins + \"{instruction}\" + tok_res\n",
        "\n",
        "max_input_length = args.max_input_length\n",
        "max_generate_length = args.max_generate_length\n",
        "generation_kwargs = {\n",
        "    \"top_p\": args.top_p,\n",
        "    \"temperature\": args.temperature,\n",
        "    \"max_length\": max_generate_length,\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    \"pad_token_id\": tokenizer.pad_token_id,\n",
        "    \"early_stopping\": True,\n",
        "    \"no_repeat_ngram_size\": args.no_repeat_ngram_size,\n",
        "}\n",
        "\n",
        "sess_text = \"\"\n",
        "while True:\n",
        "    raw_text = input(\"prompt(\\\"exit\\\" to end, \\\"clear\\\" to clear session) >>> \")\n",
        "    if not raw_text:\n",
        "        print('prompt should not be empty!')\n",
        "        continue\n",
        "    if raw_text.strip() == \"exit\":\n",
        "        print('session ended.')\n",
        "        break\n",
        "    if raw_text.strip() == \"clear\":\n",
        "        print('session cleared.')\n",
        "        sess_text = \"\"\n",
        "        continue\n",
        "    query_text = raw_text.strip()\n",
        "    sess_text += tok_ins + query_text\n",
        "    input_text = prompt_input.format_map({'instruction': sess_text.split(tok_ins, 1)[1]})\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=max_input_length).to(\n",
        "        DEV)\n",
        "    input_length = input_ids.shape[1]\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            input_ids,\n",
        "            **generation_kwargs\n",
        "        )\n",
        "    result = tokenizer.decode([el.item() for el in generated_ids[0][input_length:]], skip_special_tokens=True,\n",
        "                              spaces_between_special_tokens=False)\n",
        "    answer = result.rstrip(tokenizer.eos_token)\n",
        "    sess_text += tok_res + answer\n",
        "    print(\"=\" * 100)\n",
        "    print(answer)\n",
        "    print(\"=\" * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438,
          "referenced_widgets": [
            "8cbf19a8230d496492dbcd721ab9fe9a",
            "7001ddfcf2c440ceafe188c883677a60",
            "71c7020fac87476bad329b910089c8fc",
            "fed0a26045ff4d3c93299abc36227533",
            "cf1d0475bfa042c8819ef0864db9ca08",
            "6030f504be484b1ea2b33c9f90c77239",
            "fa065a92c2214b588c6846f895a8d38f",
            "dcfb3d4b310947e7b1a87a0bcf1f9866",
            "c379001cbd2d445eb7a83758c0df38ed",
            "9769fa7821514861b3cb7b3a0e01ec9a",
            "ec7e52c4518f4e2b941b8716c81c53f1",
            "d11dead90bc348b5b8acb7db30ac81e5",
            "77c45794190b4073846d5fecc21a82c4",
            "91c76278503f4c9b87b4a6337a8cba23",
            "752ccd08dd254efca59df6c9da0c896d",
            "58ce3dbf8f074585bddce2a10b725ea5",
            "291419c7a0424b64834f733214b0363c",
            "b0349369fd4a42119cd5dbe5b40b3d4f",
            "2d61ec87fff24355bada9abb9ef4fa16",
            "465a150a78144d22a1700d8a8ca3cf73",
            "bcb0a64b308a4d66b7a144c69cac9a53",
            "31bd7228c1134007b2808fd515b5b213",
            "5d6fa9649c904cf49f65638ac11b4b53",
            "205bea284dc04634991de1fe7f152b8c",
            "8fe684f40da24ba29419f74e53ebd7ba",
            "c5f26bd1c8784d2aac455b806fd1a407",
            "7807eea052f04be39b417f753b5b15e4",
            "712f0e976b1045eeacd1767dd348e60a",
            "1dcf50602cee4991832bbda3d5097cf2",
            "8c04328156464a1bbde0cdca27173b64",
            "df1f28ebf91d4526a6a109e7e32da24f",
            "6e0e3c38cd294b7882b690ecea84d00a",
            "f70e268d695e4934a120d7393103cd4d"
          ]
        },
        "id": "b-Hh6idY0myG",
        "outputId": "6d39c6bd-904f-41c5-af0e-4e1ef4ca123f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model ...\n",
            "['tigerbot-7b-4bit-128g.pt']\n",
            "Found 4 unique KN Linear values.\n",
            "Warming up autotune cache ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:56<00:00,  4.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "Using the following device map for the model: {'': 0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/283 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cbf19a8230d496492dbcd721ab9fe9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d11dead90bc348b5b8acb7db30ac81e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d6fa9649c904cf49f65638ac11b4b53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt(\"exit\" to end, \"clear\" to clear session) >>> 写⼀个电⼦邮件回复，告诉对⽅你会出席他们的会议并询问有关细节。 收到来⾃张三的电⼦邮件邀请，参加下周五在希尔顿酒店举⾏的会议\n",
            "====================================================================================================\n",
            "尊敬的张三，\n",
            "非常感谢您邀请我参加下周五举行的会议。我很高兴能够参加，并期待与您和其他与会者交流。\n",
            "请问会议的具体时间、地点和议程安排是什么？此外，您是否需要我提供任何特殊准备或材料？\n",
            "再次感谢您邀请我来参加会议，我期待着与您见面。\n",
            "祝好，\n",
            "[你的名字]\n",
            "====================================================================================================\n"
          ]
        }
      ]
    }
  ]
}